{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fc2b07-d6e3-4091-9918-7e6b2c2855af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "port –∞–∂–∏–ª–ª–∞–∂ –±–∞–π–≥–∞–∞ https://127.0.0.1:1054\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:1054/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x77025f597260>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built inverted index with 535 unique terms\n",
      "Built TF-IDF matrix: (20, 200)\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "\n",
    "# DOCUMENT –¶–£–ì–õ–£–£–õ–ê–• –•–≠–°–≠–ì \n",
    "\n",
    "class DocumentCollection:\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.inverted_index = defaultdict(list)\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.scraping_log = []\n",
    "        \n",
    "    def scrape_website(self, url):\n",
    "        \"\"\"Scrape using requests - NO SELENIUM\"\"\"\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "            self.scraping_log.append(f\"üì• Trying: {url}\")\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "                element.decompose()\n",
    "            \n",
    "            initial_count = len(self.documents)\n",
    "\n",
    "            articles = soup.find_all('article')\n",
    "            if articles:\n",
    "                self.scraping_log.append(f\"‚úì Found {len(articles)} articles\")\n",
    "                for article in articles[:20]:\n",
    "                    text = article.get_text(separator=' ', strip=True)\n",
    "                    if len(text) > 100:\n",
    "                        self.documents.append({\n",
    "                            'id': len(self.documents),\n",
    "                            'title': f\"Document {len(self.documents)+1}\",\n",
    "                            'content': text,\n",
    "                            'url': url,\n",
    "                            'source': 'web_scrape'\n",
    "                        })\n",
    "            \n",
    "            if len(self.documents) == initial_count:\n",
    "                paragraphs = soup.find_all('p')\n",
    "                self.scraping_log.append(f\"‚úì –û–ª—Å–æ–Ω {len(paragraphs)} paragraphs\")\n",
    "                for p in paragraphs[:30]:\n",
    "                    text = p.get_text(strip=True)\n",
    "                    if len(text) > 100:\n",
    "                        self.documents.append({\n",
    "                            'id': len(self.documents),\n",
    "                            'title': f\"Document {len(self.documents)+1}\",\n",
    "                            'content': text,\n",
    "                            'source': 'web_scrape'\n",
    "                        })\n",
    "            \n",
    "            docs_added = len(self.documents) - initial_count\n",
    "            if docs_added > 0:\n",
    "                self.scraping_log.append(f\"‚úÖ –ê—á–∞–∞–ª–ª–∞—Å–∞–Ω {docs_added} –±–∞—Ä–∏–º—Ç –º—ç–¥—ç—ç–ª—ç–ª\")\n",
    "            else:\n",
    "                self.scraping_log.append(f\"‚ùå –ö–æ–Ω—Ç–µ–Ω—Ç –æ–ª–¥—Å–æ–Ω–≥“Ø–π\")\n",
    "            return docs_added\n",
    "        except Exception as e:\n",
    "            self.scraping_log.append(f\"‚ùå –ê–ª–¥–∞–∞: {str(e)}\")\n",
    "            return 0\n",
    "    \n",
    "    def add_sample_documents(self):\n",
    "        samples = [\n",
    "            \"Information retrieval is the science of searching for information in documents and databases.\",\n",
    "            \"Machine learning algorithms can improve search engine relevance and ranking.\",\n",
    "            \"Natural language processing helps computers understand human language and text.\",\n",
    "            \"TF-IDF measures term importance by frequency and inverse document frequency.\",\n",
    "            \"Vector space models represent documents as vectors in high dimensional space.\",\n",
    "            \"Boolean retrieval uses AND OR NOT operators for precise query matching.\",\n",
    "            \"PageRank algorithm ranks web pages based on link structure and importance.\",\n",
    "            \"Precision and recall are key metrics for evaluating information retrieval systems.\",\n",
    "            \"Query expansion improves search results by adding related terms to queries.\",\n",
    "            \"Inverted indexes enable fast full-text search across large document collections.\"\n",
    "        ]\n",
    "        for idx, text in enumerate(samples):\n",
    "            self.documents.append({\n",
    "                'id': len(self.documents),\n",
    "                'title': f\"Sample Document {idx+1}\",\n",
    "                'content': text,\n",
    "                'source': 'sample'\n",
    "            })\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        stopwords = {'the', 'is', 'at', 'which', 'on', 'a', 'an', 'and', 'or', 'but', 'in', \n",
    "                    'with', 'to', 'for', 'of', 'as', 'by', 'that', 'this', 'it', 'from', \n",
    "                    'are', 'was', 'were', 'been', 'be', 'have', 'has', 'had', 'do', 'does', \n",
    "                    'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can'}\n",
    "        return [w for w in words if len(w) > 2 and w not in stopwords]\n",
    "    \n",
    "    def build_inverted_index(self):\n",
    "        self.inverted_index = defaultdict(list)\n",
    "        for doc in self.documents:\n",
    "            tokens = self.preprocess_text(doc['content'])\n",
    "            for position, token in enumerate(tokens):\n",
    "                self.inverted_index[token].append((doc['id'], position))\n",
    "        print(f\"Built inverted index with {len(self.inverted_index)} unique terms\")\n",
    "    \n",
    "    def build_tfidf_matrix(self):\n",
    "        if not self.documents:\n",
    "            return\n",
    "        corpus = [doc['content'] for doc in self.documents]\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=200, stop_words='english', ngram_range=(1,2), min_df=1\n",
    "        )\n",
    "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(corpus)\n",
    "        print(f\"Built TF-IDF matrix: {self.tfidf_matrix.shape}\")\n",
    "\n",
    "\n",
    "# SEARCH ENGINE –¢–û–•–ò–†–ì–û–û–ù–ò–ô –Ø–î–ê–†–ì–ê–ê–¢–ê–ô –•–≠–°–≠–ì\n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(self, doc_collection):\n",
    "        self.collection = doc_collection\n",
    "        \n",
    "    def boolean_search(self, query):\n",
    "        terms = self.collection.preprocess_text(query)\n",
    "        if not terms or terms[0] not in self.collection.inverted_index:\n",
    "            return []\n",
    "        result_docs = set(doc_id for doc_id, _ in self.collection.inverted_index[terms[0]])\n",
    "        for term in terms[1:]:\n",
    "            if term in self.collection.inverted_index:\n",
    "                term_docs = set(doc_id for doc_id, _ in self.collection.inverted_index[term])\n",
    "                result_docs = result_docs.intersection(term_docs)\n",
    "            else:\n",
    "                return []\n",
    "        return list(result_docs)\n",
    "    \n",
    "    def vector_space_search(self, query, top_k=10):\n",
    "        if self.collection.tfidf_matrix is None:\n",
    "            return []\n",
    "        query_vector = self.collection.tfidf_vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_vector, self.collection.tfidf_matrix).flatten()\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] > 0:\n",
    "                results.append({\n",
    "                    'doc_id': idx,\n",
    "                    'score': float(similarities[idx]),\n",
    "                    'title': self.collection.documents[idx]['title'],\n",
    "                    'content': self.collection.documents[idx]['content'][:200] + \"...\"\n",
    "                })\n",
    "        return results\n",
    "    \n",
    "    def query_expansion_rocchio(self, query, relevant_docs, alpha=1.0, beta=0.75):\n",
    "        if not relevant_docs or self.collection.tfidf_matrix is None:\n",
    "            return query\n",
    "        query_vector = self.collection.tfidf_vectorizer.transform([query]).toarray()[0]\n",
    "        relevant_vectors = self.collection.tfidf_matrix[relevant_docs].toarray()\n",
    "        relevant_centroid = np.mean(relevant_vectors, axis=0)\n",
    "        new_query_vector = alpha * query_vector + beta * relevant_centroid\n",
    "        feature_names = self.collection.tfidf_vectorizer.get_feature_names_out()\n",
    "        top_indices = new_query_vector.argsort()[-10:][::-1]\n",
    "        expanded_terms = [feature_names[i] for i in top_indices if new_query_vector[i] > 0]\n",
    "        return \" \".join(expanded_terms[:5])\n",
    "\n",
    "\n",
    "# EVALUATOR –•–ò–ô–•\n",
    "\n",
    "class Evaluator:\n",
    "    @staticmethod\n",
    "    def precision_at_k(retrieved, relevant, k):\n",
    "        retrieved_at_k = retrieved[:k]\n",
    "        relevant_retrieved = len(set(retrieved_at_k).intersection(set(relevant)))\n",
    "        return relevant_retrieved / k if k > 0 else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(retrieved, relevant, k):\n",
    "        retrieved_at_k = retrieved[:k]\n",
    "        relevant_retrieved = len(set(retrieved_at_k).intersection(set(relevant)))\n",
    "        return relevant_retrieved / len(relevant) if len(relevant) > 0 else 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def f1_score(precision, recall):\n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_precision(retrieved, relevant):\n",
    "        if not relevant:\n",
    "            return 0\n",
    "        score, num_hits = 0.0, 0.0\n",
    "        for i, doc_id in enumerate(retrieved):\n",
    "            if doc_id in relevant:\n",
    "                num_hits += 1.0\n",
    "                score += num_hits / (i + 1.0)\n",
    "        return score / len(relevant)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(retrieved, relevant, k):\n",
    "        def dcg(scores, k):\n",
    "            return sum(score / math.log2(i + 2) for i, score in enumerate(scores[:k]))\n",
    "        retrieved_at_k = retrieved[:k]\n",
    "        relevance_scores = [1 if doc in relevant else 0 for doc in retrieved_at_k]\n",
    "        ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "        dcg_value = dcg(relevance_scores, k)\n",
    "        idcg_value = dcg(ideal_scores, k)\n",
    "        return dcg_value / idcg_value if idcg_value > 0 else 0\n",
    "\n",
    "\n",
    "# COMPONENT –•–≠–°–≠–ì \n",
    "\n",
    "doc_collection = DocumentCollection()\n",
    "search_engine = None\n",
    "evaluator = Evaluator()\n",
    "initialization_done = False\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        html.H1(\"–ú—ç–¥—ç—ç–ª–ª–∏–π–Ω —Ö–∞–π–ª—Ç—ã–Ω —Å–∏—Å—Ç–µ–º By - Ganaa\", \n",
    "                style={'textAlign': 'center', 'color': '#2c3e50'}),\n",
    "        html.P(\"–ò–Ω–¥–µ–∫—Å–∂“Ø“Ø–ª—ç–ª—Ç, –ñ–∞–≥—Å–∞–∞–ª—Ç, Query expansion, and evaluation “Æ–∑“Ø“Ø–ª—ç–ª—Ç“Ø“Ø–¥–∏–π–≥ —Ö–∞–º—Ä—É—É–ª—Å–∞–Ω IR —Å–∏—Å—Ç–µ–º.\",\n",
    "               style={'textAlign': 'center', 'color': '#7f8c8d', 'fontSize': '14px'})\n",
    "    ], style={'backgroundColor': '#ecf0f1', 'padding': '20px', 'marginBottom': '20px'}),\n",
    "    \n",
    "    html.Div([\n",
    "        html.H3(\"üîß –ê–ª—Ö–∞–º 1: C–∏—Å—Ç–µ–º–∏–π–Ω –∞—á–∞–∞–ª–ª–∞—Ö\"),\n",
    "        html.Button(\"Document-–≥ –∞—á–∞–∞–ª–ª–∞—Ö & Index “Ø“Ø—Å–≥—ç—Ö\", id='init-button', \n",
    "                   style={'padding': '10px 20px', 'fontSize': '16px', 'cursor': 'pointer'}),\n",
    "        html.Div(id='init-status', style={'marginTop': '10px', 'color': '#27ae60'})\n",
    "    ], style={'padding': '20px', 'backgroundColor': '#fff', 'marginBottom': '20px', 'borderRadius': '5px'}),\n",
    "    \n",
    "    html.Div([\n",
    "        html.H3(\"üîç –ê–ª—Ö–∞–º 2: –ë–∞—Ä–∏–º—Ç–∏–π–≥ —Ö–∞–π—Ö\"),\n",
    "        dcc.Input(id='query-input', type='text', placeholder='Enter your search query...',\n",
    "                 style={'width': '60%', 'padding': '10px', 'fontSize': '14px'}),\n",
    "        html.Button(\"Search\", id='search-button', \n",
    "                   style={'marginLeft': '10px', 'padding': '10px 20px', 'cursor': 'pointer'}),\n",
    "        \n",
    "        html.Div([\n",
    "            html.Label(\"–ê—à–∏–≥–ª–∞—Ö –∞—Ä–≥—É—É–¥:\", style={'marginRight': '10px', 'fontWeight': 'bold'}),\n",
    "            dcc.RadioItems(\n",
    "                id='search-method',\n",
    "                options=[\n",
    "                    {'label': ' Boolean (AND)', 'value': 'boolean'},\n",
    "                    {'label': ' Vector Space (TF-IDF)', 'value': 'vector'}\n",
    "                ],\n",
    "                value='vector',\n",
    "                inline=True\n",
    "            )\n",
    "        ], style={'marginTop': '15px'}),\n",
    "        \n",
    "        html.Div(id='search-results', style={'marginTop': '20px'})\n",
    "    ], style={'padding': '20px', 'backgroundColor': '#fff', 'marginBottom': '20px', 'borderRadius': '5px'}),\n",
    "    \n",
    "    html.Div([\n",
    "        html.H3(\"üéØ –ê–ª—Ö–∞–º 3: Query Expansion (Rocchio)\"),\n",
    "        html.P(\"–î—ç—ç—Ä—Ö —Ö–∞–π–ª—Ç—ã–Ω “Ø—Ä –¥“Ø–Ω–≥—ç—ç—Å —Ö–æ–ª–±–æ–≥–¥–æ—Ö –±–∞—Ä–∏–º—Ç –±–∏—á–≥“Ø“Ø–¥–∏–π–≥ —Å–æ–Ω–≥–æ–æ–¥, –¥–∞—Ä–∞–∞ –Ω—å –∞—Å—É—É–ª–≥—ã–≥ ”©—Ä–≥”©–∂“Ø“Ø–ª—ç—Ö–∏–π–Ω —Ç—É–ª–¥ —ç–Ω–¥ –¥–∞—Ä–Ω–∞ —É—É:\"),\n",
    "        dcc.Input(id='relevant-docs', type='text', placeholder='Enter relevant doc IDs (comma-separated, e.g., 0,2,5)',\n",
    "                 style={'width': '60%', 'padding': '10px'}),\n",
    "        html.Button(\"Expand Query\", id='expand-button', \n",
    "                   style={'marginLeft': '10px', 'padding': '10px 20px', 'cursor': 'pointer'}),\n",
    "        html.Div(id='expanded-query', style={'marginTop': '10px'})\n",
    "    ], style={'padding': '20px', 'backgroundColor': '#fff', 'marginBottom': '20px', 'borderRadius': '5px'}),\n",
    "    \n",
    "    html.Div([\n",
    "        html.H3(\"üìä –ê–ª—Ö–∞–º 4: “Æ–Ω—ç–ª–≥—ç—ç–Ω–∏–π “Ø–∑“Ø“Ø–ª—ç–ª—Ç“Ø“Ø–¥\"),\n",
    "        html.P(\"–û–¥–æ–æ–≥–∏–π–Ω –∞—Å—É—É–ª–≥–∞–¥ “Ø–Ω–¥—ç—Å–ª—ç–ª—Ç—ç–π —Ö–æ–ª–±–æ–æ—Ç–æ–π –±–∞—Ä–∏–º—Ç –±–∏—á–≥–∏–π–Ω –¥—É–≥–∞–∞—Ä—ã–≥ –æ—Ä—É—É–ª–Ω–∞ —É—É:\"),\n",
    "        dcc.Input(id='ground-truth', type='text', placeholder='Relevant doc IDs (e.g., 0,1,3)',\n",
    "                 style={'width': '60%', 'padding': '10px'}),\n",
    "        html.Button(\"Calculate Metrics\", id='eval-button', \n",
    "                   style={'marginLeft': '10px', 'padding': '10px 20px', 'cursor': 'pointer'}),\n",
    "        html.Div(id='metrics-display', style={'marginTop': '20px'})\n",
    "    ], style={'padding': '20px', 'backgroundColor': '#fff', 'marginBottom': '20px', 'borderRadius': '5px'}),\n",
    "    \n",
    "    html.Div([\n",
    "        html.H3(\"üß™ –ê–ª—Ö–∞–º 5: –¢—É—Ä—à–∏–ª—Ç—ã–≥ —ç—Ö–ª“Ø“Ø–ª—ç—Ö\"),\n",
    "        html.Button(\"–ê—Ä–≥—É—É–¥—ã–≥ —Ö–∞—Ä—å—Ü—É—É–ª–∞—Ö (Boolean vs Vector Space vs Expanded)\", \n",
    "                   id='experiment-button',\n",
    "                   style={'padding': '10px 20px', 'cursor': 'pointer'}),\n",
    "        dcc.Graph(id='experiment-chart', style={'marginTop': '20px'})\n",
    "    ], style={'padding': '20px', 'backgroundColor': '#fff', 'marginBottom': '20px', 'borderRadius': '5px'}),\n",
    "    \n",
    "    html.Div([\n",
    "        html.H3(\"üìà Word Cloud & TF-IDF –ì—Ä–∞—Ñ–∏–∫\"),\n",
    "        html.Img(id='wordcloud-img', style={'maxWidth': '800px', 'width': '100%'}),\n",
    "        dcc.Graph(id='tfidf-chart')\n",
    "    ], style={'padding': '20px', 'backgroundColor': '#fff', 'borderRadius': '5px'})\n",
    "    \n",
    "], style={'maxWidth': '1200px', 'margin': '0 auto', 'padding': '20px', 'backgroundColor': '#f5f5f5'})\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('init-status', 'children'),\n",
    "    Input('init-button', 'n_clicks'),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def initialize_system(n_clicks):\n",
    "    global search_engine, initialization_done\n",
    "    \n",
    "    if initialization_done:\n",
    "        return \"‚úÖ –°–∏—Å—Ç–µ–º –∞–ª—å —Ö—ç–¥–∏–π–Ω –∞–∂–∏–ª–ª–∞–∂ –±–∞–π–Ω–∞!\"\n",
    "    \n",
    "    urls_to_try = [\n",
    "        'https://en.wikipedia.org/wiki/Information_retrieval',\n",
    "        'https://en.wikipedia.org/wiki/Machine_learning',\n",
    "        'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
    "    ]\n",
    "    \n",
    "    total_docs = 0\n",
    "    doc_collection.scraping_log = []\n",
    "    \n",
    "    for url in urls_to_try:\n",
    "        num_docs = doc_collection.scrape_website(url)\n",
    "        total_docs += num_docs\n",
    "        if total_docs >= 20:  \n",
    "            break\n",
    "    \n",
    "    if total_docs == 0:\n",
    "        print(\"‚ö†Ô∏è –í–µ–±—ç—ç—Å —Ö—É—É–ª–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π, –∂–∏—à—ç—ç ”©–≥”©–≥–¥–ª–∏–π–≥ —É–Ω—à–∏–∂ –±–∞–π–Ω–∞...\")\n",
    "        doc_collection.add_sample_documents()\n",
    "        total_docs = len(doc_collection.documents)\n",
    "        message_suffix = \"(using sample documents - scraping failed)\"\n",
    "    else:\n",
    "        message_suffix = \"(scraped from web)\"\n",
    "    \n",
    "    # –ò–ù–î–ï–ö–° “Æ“Æ–°–ì–≠–•\n",
    "    doc_collection.build_inverted_index()\n",
    "    doc_collection.build_tfidf_matrix()\n",
    "    \n",
    "    search_engine = SearchEngine(doc_collection)\n",
    "    initialization_done = True\n",
    "    \n",
    "    log_display = html.Div([\n",
    "        html.P(f\"‚úÖ –°–∏—Å—Ç–µ–º–∏–π–≥ –∞–∂–∏–ª–ª–∞–∂ —ç—Ö—ç–ª—Å—ç–Ω! –ê—á–∞–∞–ª–ª–∞—Å–∞–Ω {total_docs} –±–∞—Ä–∏–º—Ç—É—É–¥ {message_suffix}\"),\n",
    "        html.Details([\n",
    "            html.Summary(\"Scrap-–Ω –ª–æ–≥–∏–π–≥ —Ö–∞—Ä–∞—Ö\", style={'cursor': 'pointer', 'color': '#3498db'}),\n",
    "            html.Div([html.P(log, style={'fontSize': '12px', 'margin': '2px'}) \n",
    "                     for log in doc_collection.scraping_log])\n",
    "        ], style={'marginTop': '10px'})\n",
    "    ])\n",
    "    \n",
    "    return log_display\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('search-results', 'children'),\n",
    "    [Input('search-button', 'n_clicks')],\n",
    "    [State('query-input', 'value'),\n",
    "     State('search-method', 'value')],\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def perform_search(n_clicks, query, method):\n",
    "    if not initialization_done or not query:\n",
    "        return \"‚ö†Ô∏è –°–∏—Å—Ç–µ–º–∏–π–≥ –∞—á–∞–∞–ª–ª–∞—Å–Ω—ã –¥–∞—Ä–∞–∞ –∞—Å—É—É–ª–≥–∞–∞ –æ—Ä—É—É–ª–Ω–∞ —É—É.\"\n",
    "    \n",
    "    if method == 'boolean':\n",
    "        doc_ids = search_engine.boolean_search(query)\n",
    "        results = [{'doc_id': doc_id, \n",
    "                   'title': doc_collection.documents[doc_id]['title'],\n",
    "                   'content': doc_collection.documents[doc_id]['content'][:200] + \"...\"}\n",
    "                  for doc_id in doc_ids[:10]]\n",
    "    else:  \n",
    "        results = search_engine.vector_space_search(query, top_k=10)\n",
    "    \n",
    "    if not results:\n",
    "        return html.Div([\n",
    "            html.P(\"–•–∞–π–ª—Ç –∏–ª—ç—Ä—Å—ç–Ω–≥“Ø–π.\", style={'color': '#e74c3c', 'fontWeight': 'bold'}),\n",
    "            html.P(f\"–•–∞–π—Å–∞–Ω: '{query}'\"),\n",
    "            html.P(\"”®”©—Ä —Ç“Ø–ª—Ö“Ø“Ø—Ä “Ø–≥ —Å–æ–Ω–≥–æ–Ω–æ —É—É.\")\n",
    "        ])\n",
    "    \n",
    "\n",
    "    result_divs = [html.H4(f\"–û–ª—Å–æ–Ω {len(results)} “Ø—Ä –¥“Ø–Ω:\")]\n",
    "    for i, res in enumerate(results):\n",
    "        score_text = f\" (–û–Ω–æ–æ: {res['score']:.3f})\" if 'score' in res else \"\"\n",
    "        result_divs.append(\n",
    "            html.Div([\n",
    "                html.H4(f\"{i+1}. –ë–∞—Ä–∏–º—Ç—ã–Ω ID: {res['doc_id']} - {res['title']}{score_text}\"),\n",
    "                html.P(res['content'], style={'color': '#555'})\n",
    "            ], style={'borderBottom': '1px solid #ddd', 'paddingBottom': '10px', 'marginBottom': '10px'})\n",
    "        )\n",
    "    \n",
    "    return html.Div(result_divs)\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('expanded-query', 'children'),\n",
    "    [Input('expand-button', 'n_clicks')],\n",
    "    [State('query-input', 'value'),\n",
    "     State('relevant-docs', 'value')],\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def expand_query(n_clicks, query, relevant_docs_str):\n",
    "    if not initialization_done or not query or not relevant_docs_str:\n",
    "        return \"‚ö†Ô∏è –ê—Å—É—É–ª–≥–∞ –±–æ–ª–æ–Ω —Ö–æ–ª–±–æ–≥–¥–æ—Ö –±–∞—Ä–∏–º—Ç –±–∏—á–≥–∏–π–Ω ID-–≥ –æ—Ä—É—É–ª–Ω–∞ —É—É.\"\n",
    "    \n",
    "    try:\n",
    "        relevant_docs = [int(x.strip()) for x in relevant_docs_str.split(',')]\n",
    "        expanded = search_engine.query_expansion_rocchio(query, relevant_docs)\n",
    "        \n",
    "        return html.Div([\n",
    "            html.P(f\"Original Query: {query}\", style={'fontWeight': 'bold'}),\n",
    "            html.P(f\"Expanded Query: {expanded}\", style={'color': '#27ae60', 'fontWeight': 'bold'})\n",
    "        ])\n",
    "    except:\n",
    "        return \"‚ö†Ô∏è –ë–∞—Ä–∏–º—Ç—ã–Ω ID —Ö–∞—è–≥–∏–π–Ω –¥—É–≥–∞–∞—Ä –±—É—Ä—É—É –±–∞–π–Ω–∞. –¢–æ–æ–≥ —Ç–∞—Å–ª–∞–ª—Ç–∞–π –æ—Ä—É—É–ª–Ω–∞ —É—É. (e.g., 0,2,5)\"\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('metrics-display', 'children'),\n",
    "    [Input('eval-button', 'n_clicks')],\n",
    "    [State('query-input', 'value'),\n",
    "     State('search-method', 'value'),\n",
    "     State('ground-truth', 'value')],\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def calculate_metrics(n_clicks, query, method, ground_truth_str):\n",
    "    if not initialization_done or not query or not ground_truth_str:\n",
    "        return \"‚ö†Ô∏è Enter query and ground truth relevant documents.\"\n",
    "    \n",
    "    try:\n",
    "        ground_truth = [int(x.strip()) for x in ground_truth_str.split(',')]\n",
    "        \n",
    "\n",
    "        if method == 'boolean':\n",
    "            retrieved = search_engine.boolean_search(query)\n",
    "        else:\n",
    "            results = search_engine.vector_space_search(query, top_k=10)\n",
    "            retrieved = [r['doc_id'] for r in results]\n",
    "        \n",
    "\n",
    "        k_values = [5, 10]\n",
    "        metrics = {}\n",
    "        \n",
    "        for k in k_values:\n",
    "            p = evaluator.precision_at_k(retrieved, ground_truth, k)\n",
    "            r = evaluator.recall_at_k(retrieved, ground_truth, k)\n",
    "            f1 = evaluator.f1_score(p, r)\n",
    "            ndcg = evaluator.ndcg_at_k(retrieved, ground_truth, k)\n",
    "            \n",
    "            metrics[k] = {'precision': p, 'recall': r, 'f1': f1, 'ndcg': ndcg}\n",
    "        \n",
    "        ap = evaluator.average_precision(retrieved, ground_truth)\n",
    "        \n",
    "\n",
    "        return html.Div([\n",
    "            html.H4(\"Evaluation “Ø—Ä –¥“Ø–Ω:\", style={'color': '#2c3e50'}),\n",
    "            html.Table([\n",
    "                html.Tr([html.Th(\"“Æ–∑“Ø“Ø–ª—ç–ª—Ç\"), html.Th(\"@5\"), html.Th(\"@10\")],\n",
    "                       style={'backgroundColor': '#ecf0f1'}),\n",
    "                html.Tr([html.Td(\"Precision\"), \n",
    "                        html.Td(f\"{metrics[5]['precision']:.3f}\"),\n",
    "                        html.Td(f\"{metrics[10]['precision']:.3f}\")]),\n",
    "                html.Tr([html.Td(\"Recall\"), \n",
    "                        html.Td(f\"{metrics[5]['recall']:.3f}\"),\n",
    "                        html.Td(f\"{metrics[10]['recall']:.3f}\")]),\n",
    "                html.Tr([html.Td(\"F1 Score\"), \n",
    "                        html.Td(f\"{metrics[5]['f1']:.3f}\"),\n",
    "                        html.Td(f\"{metrics[10]['f1']:.3f}\")]),\n",
    "                html.Tr([html.Td(\"NDCG\"), \n",
    "                        html.Td(f\"{metrics[5]['ndcg']:.3f}\"),\n",
    "                        html.Td(f\"{metrics[10]['ndcg']:.3f}\")])\n",
    "            ], style={'width': '100%', 'borderCollapse': 'collapse', 'marginTop': '10px',\n",
    "                     'border': '1px solid #ddd'}),\n",
    "            html.P(f\"Mean Average Precision (MAP): {ap:.3f}\", \n",
    "                  style={'marginTop': '15px', 'fontWeight': 'bold', 'color': '#2c3e50'})\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Error: {str(e)}\"\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('experiment-chart', 'figure'),\n",
    "    Input('experiment-button', 'n_clicks'),\n",
    "    State('query-input', 'value'),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def run_experiments(n_clicks, query):\n",
    "    if not initialization_done or not query:\n",
    "        return {}\n",
    "    \n",
    "    vector_results = search_engine.vector_space_search(query, top_k=10)\n",
    "    ground_truth = [r['doc_id'] for r in vector_results[:3]]\n",
    "    \n",
    "    methods = {}\n",
    "    \n",
    "    bool_results = search_engine.boolean_search(query)\n",
    "    methods['Boolean AND'] = {\n",
    "        'precision': evaluator.precision_at_k(bool_results, ground_truth, 10),\n",
    "        'recall': evaluator.recall_at_k(bool_results, ground_truth, 10),\n",
    "        'ndcg': evaluator.ndcg_at_k(bool_results, ground_truth, 10)\n",
    "    }\n",
    "    \n",
    "    vec_retrieved = [r['doc_id'] for r in vector_results]\n",
    "    methods['Vector Space'] = {\n",
    "        'precision': evaluator.precision_at_k(vec_retrieved, ground_truth, 10),\n",
    "        'recall': evaluator.recall_at_k(vec_retrieved, ground_truth, 10),\n",
    "        'ndcg': evaluator.ndcg_at_k(vec_retrieved, ground_truth, 10)\n",
    "    }\n",
    "    \n",
    "    if vector_results:\n",
    "        relevant_docs = [vector_results[0]['doc_id']]  # Use top result\n",
    "        expanded_query = search_engine.query_expansion_rocchio(query, relevant_docs)\n",
    "        exp_results = search_engine.vector_space_search(expanded_query, top_k=10)\n",
    "        exp_retrieved = [r['doc_id'] for r in exp_results]\n",
    "        methods['Query Expansion'] = {\n",
    "            'precision': evaluator.precision_at_k(exp_retrieved, ground_truth, 10),\n",
    "            'recall': evaluator.recall_at_k(exp_retrieved, ground_truth, 10),\n",
    "            'ndcg': evaluator.ndcg_at_k(exp_retrieved, ground_truth, 10)\n",
    "        }\n",
    "    \n",
    "    method_names = list(methods.keys())\n",
    "    metrics_list = ['precision', 'recall', 'ndcg']\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for metric in metrics_list:\n",
    "        values = [methods[m][metric] for m in method_names]\n",
    "        fig.add_trace(go.Bar(name=metric.upper(), x=method_names, y=values))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"–•–∞–π–ª—Ç—ã–Ω –∞—Ä–≥—É—É–¥—ã–Ω —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç\",\n",
    "        xaxis_title=\"Method\",\n",
    "        yaxis_title=\"Score\",\n",
    "        barmode='group',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    [Output('wordcloud-img', 'src'),\n",
    "     Output('tfidf-chart', 'figure')],\n",
    "    Input('init-button', 'n_clicks'),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def update_visualizations(n_clicks):\n",
    "    if not initialization_done:\n",
    "        return '', {}\n",
    "    \n",
    "    all_text = ' '.join([doc['content'] for doc in doc_collection.documents])\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "    \n",
    "    if not os.path.exists('assets'):\n",
    "        os.makedirs('assets')\n",
    "    \n",
    "    wordcloud_path = 'assets/wordcloud.png'\n",
    "    wordcloud.to_file(wordcloud_path)\n",
    "    \n",
    "    if doc_collection.tfidf_matrix is not None:\n",
    "        feature_names = doc_collection.tfidf_vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = doc_collection.tfidf_matrix.sum(axis=0).A1\n",
    "        top_indices = tfidf_scores.argsort()[-15:][::-1]\n",
    "        \n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        top_scores = [tfidf_scores[i] for i in top_indices]\n",
    "        \n",
    "        fig = px.bar(x=top_words, y=top_scores, \n",
    "                    labels={'x': 'Terms', 'y': 'TF-IDF Score'},\n",
    "                    title='Top 15 Terms by TF-IDF Score')\n",
    "        fig.update_layout(xaxis_tickangle=-45)\n",
    "        \n",
    "        return f'/assets/wordcloud.png?t={int(time.time())}', fig\n",
    "    \n",
    "    return f'/assets/wordcloud.png?t={int(time.time())}', {}\n",
    "\n",
    "\n",
    "# RUN \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    print(\"port –∞–∂–∏–ª–ª–∞–∂ –±–∞–π–≥–∞–∞ –¥–∞—Ä–∞–∞–¥ –æ—Ä–æ–æ—Ä–æ–π https://127.0.0.1:1054\")\n",
    "    \n",
    "    app.run(debug=True, port=1054)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
